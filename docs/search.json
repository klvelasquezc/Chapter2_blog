{
  "articles": [
    {
      "path": "ANDEVA.html",
      "title": "Script ANDEVA de una via",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nConceptos básicos del análisis de la varianza\r\n\r\nC.1\r\nR.2\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nConceptos básicos del análisis de la varianza\r\nLa hipótesis nula en un análisis de la varianza tipo I común es:\r\n\\[H0: m1 = m2 = m3 = ... = mk\\]\r\n¿Cómo es que esta hipótesis se pone a prueba en un ANDEVA? Por cierto esta es una prueba “omnibus”, es decir ¡prueba muchas cosas de un jalón!\r\nPara ver como es que opera el anova veamos el ejemplo que sigue: Tomemos un solo factor, “f”, con dos niveles y pongamos los datos en una gráfica simple, según el orden en el que fueron obtenidas las mediciones.\r\n\r\n\r\nanova<-read.table(\"anova.data.txt\",header=T)\r\nattach(anova)\r\nnames(anova)\r\n\r\n\r\n\r\n\r\n\r\nplot(y)\r\nabline(mean(y), 0, col=4)\r\nfor (i in 1:length(y)) lines (c(i,i), c(mean(y), y[i]), col=5)\r\n\r\n\r\n\r\n\r\n¿Qué muestra esta gráfica? ¿A que equivale la suma de los trazos verticales?\r\n\\[ SCT=\\Sigma(y- \\overline{y})^2\\]\r\nAhora vamos a hacer exactamente lo mismo, pero ahora dividiendo por cada uno de los niveles del factor F. Incorporemos la información del factor “f”. Para esto hay que calcular los promedios de “y” que corresponden a los niveles de “f”\r\n\r\n\r\npromedios <- tapply(y, f, mean)\r\n\r\n\r\n\r\nGrafiquemos esta nueva estructura de datos sobre la gráfica que ya tenemos\r\n\r\n\r\nplot(y)\r\nlines(c(1, 7), c(promedios[1], promedios[1]), col = 2)\r\nlines(c(7, 14), c(promedios[2], promedios[2]), col = 5)\r\nfor (i in 1:7 ) lines (c(i,i), c(promedios[1], y[i]), col = 1, lty=6)\r\nfor (i in 8:14) lines (c(i,i), c(promedios[2], y[i]), col = 1, lty=6)\r\n\r\n\r\n\r\n\r\n¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos punteados verticales?\r\n\\[SCE= \\Sigma(y_1-\\hat{y_1})^2 + \\Sigma(y_2- \\hat{y_2})^2\\]\r\nSi las dos medias fueran iguales ¿cómo compararían estas dos gráficas?\r\nSi lo piensan tendrían que ser iguales porque las medias de los niveles del tratamiento se nivelarían a la misma altura. Si las medias son significativamente distintas ¿cual varianza sería mayor? la calculada con SCT o la calculada con SCE? Esta es la razón por la cual el ANDEVA compara medias a través de la comparación de varianzas!!!!\r\n¿Qué interpretación tiene la diferencia entre las dos sumas mencionadas arriba? Pues es precisamente la varianza explicada por el modelo. Esta diferencia se asocia con la siguiente gráfica:\r\n\r\n\r\nmodelo <- lm(y~f)\r\nplot (y)\r\nabline (mean(y), 0, col = 4)\r\npoints(predict(modelo), pch = 16, col = 5)\r\nfor (i in 1:14) lines(c(i, i), c(mean(y), predict(modelo)[i]), col = 6)\r\n\r\n\r\n\r\n\r\nC.1\r\nR.2\r\n¿Que implica el ajuste del modelo ANOVA del factor “f”?\r\n\r\n\r\nSCT<-sum((y-mean(y))^2)\r\nSCT\r\n\r\n\r\n[1] 55.5\r\n\r\nLa pregunta es cuanto de esta variación es explicada por diferencias entre las medias de A y B (niveles del factor F) y cuanto por el error\r\n\r\n\r\nSCEa<-sum((y[f==\"a\"]-mean(y[f==\"a\"]))^2)\r\nSCEb<-sum((y[f==\"b\"]-mean(y[f==\"b\"]))^2)\r\n\r\n\r\n\r\nEntonces la SCE es la suma de estas dos cantidades\r\n\r\n\r\nSCE<-SCEa+SCEb\r\nSCE\r\n\r\n\r\n[1] 24\r\n\r\nFinalmente la SCA es SCT-SCE\r\n\r\n\r\nSCA<-SCT-SCE\r\nSCA\r\n\r\n\r\n[1] 31.5\r\n\r\nEntonces ya podemos llenar la tabla de ANOVA\r\n#C.2\r\nR.3\r\nAhora calculemos la F\r\n\r\n\r\n31.5/2\r\n\r\n\r\n[1] 15.75\r\n\r\ny la p\r\n\r\n\r\n1-pf(15.75,1,12)\r\n\r\n\r\n[1] 0.001864103\r\n\r\nAhora el automatico\r\n\r\n\r\nmodelo<-aov(y~f)\r\n\r\n\r\n\r\n\r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nboxplot(y~f,xlab=\"factor F\",ylab=\"y\")\r\n\r\n\r\n\r\n\r\n¿Cual es la conclusión? Ahora hacemos la crítica del modelo\r\n\r\n\r\npar(mfrow=c(2,2))\r\nplot(modelo)\r\n\r\n\r\n\r\n\r\ny ahora lo ultimo\r\n\r\n\r\nA<-c(6,8,5,9,7,8,6)\r\nB<-c(9,11,8,12,10,11,9)\r\nt.test(A,B)\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  A and B\r\nt = -3.9686, df = 12, p-value = 0.001864\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -4.647028 -1.352972\r\nsample estimates:\r\nmean of x mean of y \r\n        7        10 \r\n\r\nsummary(modelo)\r\n\r\n\r\n            Df Sum Sq Mean Sq F value  Pr(>F)   \r\nf            1   31.5    31.5   15.75 0.00186 **\r\nResiduals   12   24.0     2.0                   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nLa anova es una “Generalización” de t para poder comparar mas de dos medias. En realidad F=t^2\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:11-06:00"
    },
    {
      "path": "Correlacion.html",
      "title": "Script Correlación",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nVamos a ver una serie de datos para ver si existe una relación lineal entre ellos.\r\n\r\n\r\ndata<-read.table(\"twosample.txt\",header=T)\r\nattach(data)\r\ndata\r\n\r\n\r\n\r\n\r\n\r\nplot(x,y)\r\n\r\n\r\n\r\n\r\nSe acuerdan que necesitamos primero para calcular el coeficiente de correlación de pearson? Las varianzas individuales\r\n\r\n\r\nvar(x)\r\n\r\n\r\n[1] 199.9837\r\n\r\nvar(y)\r\n\r\n\r\n[1] 977.0153\r\n\r\n¿y que más? La covarianza y estamos hechos\r\n\r\n\r\nvar(x,y)\r\n\r\n\r\n[1] 414.9603\r\n\r\nAhora calculamos r\r\n\r\n\r\nvar(x,y)/sqrt(var(x)*var(y))\r\n\r\n\r\n[1] 0.9387684\r\n\r\nAhora hagamoslo en automático\r\n\r\n\r\ncor(x,y)\r\n\r\n\r\n[1] 0.9387684\r\n\r\nY ahora hagamos la prueba de hipótesis\r\nCalculamos EE de r\r\n\r\n\r\nEEr<-((1-(cor(x,y)^2))/(length(x)-2))^0.5\r\nEEr\r\n\r\n\r\n[1] 0.05025759\r\n\r\nCalculo t de la muestra\r\n\r\n\r\nte<-cor(x,y)/EEr\r\nte\r\n\r\n\r\n[1] 18.67914\r\n\r\nCalculo t de tablas\r\n\r\n\r\nqt(0.975,47)\r\n\r\n\r\n[1] 2.011741\r\n\r\nCalculo la p\r\n\r\n\r\n2*(1-pt(18.67914,47))\r\n\r\n\r\n[1] 0\r\n\r\nAhora hagamoslo de manera automática\r\n\r\n\r\npearson<-cor.test(x,y)\r\npearson\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  x and y\r\nt = 18.679, df = 47, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 0.8934139 0.9651786\r\nsample estimates:\r\n      cor \r\n0.9387684 \r\n\r\n¿Que nos falta?. Pues no sabemos si cumplimos con los supuestos. Veamos el de normalidad\r\n\r\n\r\npar(mfrow=c(1,2))\r\nqqnorm(x, main=\"Q-Q plot x\"); qqline(x, col = 2, lty = 2)\r\nqqnorm(y, main=\"Q-Q plot y\"); qqline(y, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\n¿Que opciones tengo?.\r\nHacer una prueba de sesgo y kurtosis para ver si estas desviaciones son significativas\r\nSi son significativas, puedo intentar transformaciones o puedo utilizar muchas de las otras pruebas de correlación que son robustas a la violación de este supuesto. Vean Q y k p.76 y Crawley p.97-102.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:13-06:00"
    },
    {
      "path": "Estimacion1.html",
      "title": "Script Estimación 1 - El papel de la probabilidad",
      "author": [],
      "contents": "\r\n\r\nContents\r\nEl teorema de tendencia central\r\nR.1\r\nC.1\r\n\r\nR.2\r\nC.2\r\n\r\nR.3\r\nTarea de hoy\r\n¡Fin!\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nEl teorema de tendencia central\r\nR.1\r\nPidámosle a R que tome 10000 números al azar (0-10) y que grafique con que frecuencia eligió cada número ¿que esperan ver?\r\n\r\n\r\nnumeros<-(runif(10000)*10)\r\n\r\n\r\n\r\n\r\n\r\nhist(runif(10000)*10,main=\"\")\r\n\r\n\r\n\r\n\r\nTomemos 5 números al azar del 0 al 10 y saquemos el promedio de los cinco números ¿cual creen que sea el promedio típico? Hagámonos\r\n\r\n\r\nclase<-c((mean(,,,,)), (mean(,,,,)))\r\nclase\r\n\r\n\r\n\r\nAhora pidámosle a R que lo haga 10000 veces (¡10000 muestras de números del 0 al 10!)\r\n\r\n\r\nmeans<-numeric(10000)\r\nfor (i in 0:10000){ \r\n  means[i]<- mean(runif(5)*10)}\r\n  \r\nhist(means,ylim=c(0,1600))\r\n\r\n\r\n\r\n\r\nNoten que existe una tendencia central. Lo que significa que el mero hecho de muestrear genera una tendencia central en el valor promedio.\r\nC.1\r\nR.2\r\nEl histograma de los números se ve como una distribución normal. ¿porque no dibujamos una función normal sobre este histograma para ver que tan normal es? ¿Que necesitamos saber?\r\n\r\n\r\nmean(means)\r\n\r\n\r\n[1] 5.00833\r\n\r\nsd(means)\r\n\r\n\r\n[1] 1.284904\r\n\r\ny meterlo en la ecuación \\[ f(y) = \\frac{1/}{(\\sqrt{2 \\pi} \\sigma)} e^{y- \\mu / 2 \\sigma^2} \\] para cada valor de means verdad? R lo hace por ustedes\r\nPrimero generamos una serie de números para el eje x que le permita a R saber cada cuando poner un puntito de la línea. Si queremos una linea suave una buena regla es 100 números. Como queremos números entre 0 y 10:\r\n\r\n\r\nxv<-seq(0,10,0.1)\r\n\r\n\r\n\r\nComo la función normal tiene un área bajo la curva de 1 y nosotros tenemos 10000 valores, necesitamos escalar la curva multiplicándola por el número de valores que se encuentran de cada lado de el valor de en medio (mediana).\r\n\r\n\r\nyv<-dnorm(xv,mean=4.993275,sd=1.291482)*5000\r\n\r\n\r\n\r\n\r\n\r\nhist(means,ylim=c(0,1600))\r\nlines(xv,yv)\r\n\r\n\r\n\r\n\r\nEl ajuste es perfecto! Lo interesante del teorema de tendencia central es que no importa cual sea la distribución real, si se toma una muestra de ella, se comportará normalmente.\r\nC.2\r\nR.3\r\nEl estandarizar la curva normal tiene muchas ventajas. Nos permite por ejemplo saber cual es el área hasta cualquier valor del eje X (llamados desviaciones estándar).\r\nPidámosle a R que nos dibuje una Distribución Normal Estandarizada. Recuerden que lo primero que hay que hacer cuando se dibujan líneas\r\n\r\n\r\nnd<-seq(-3,3,0.01)\r\n\r\n\r\n\r\nCuando no le damos la media y a la var, significa que queremos la estandarizada (el default), solo le dimos las x.\r\n\r\n\r\ny<-dnorm(nd)\r\nplot(nd,y,type=\"l\")\r\n\r\n\r\n\r\n\r\nAhora utilicemos pnorm para determinar que proporción de valores caen por debajo de dos desviaciones estándar.\r\n\r\n\r\npnorm(-2)\r\n\r\n\r\n[1] 0.02275013\r\n\r\nque proporción? y ahora por debajo de 1 ds\r\n\r\n\r\npnorm(-1)\r\n\r\n\r\n[1] 0.1586553\r\n\r\n¿Que pasa si queremos saber que proporción de muestras caen A LA DERECHA de la desviación 3?\r\nesta es una opción\r\n\r\n\r\n 1-pnorm(3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\n¿y la otra?\r\n\r\n\r\npnorm(-3)\r\n\r\n\r\n[1] 0.001349898\r\n\r\nEs claro que un valor de 3 cuando la media es 0 en una dn es muy poco probable (0.13% de las veces cuando se saca una muestra de la pob.)\r\nAhora veamos un uso muy común de la distribución de z. Este es preguntarse bajo una situación aleatoria (solo por el hecho de muestrear una población) entre que desviaciones estándar podemos encontrar el 95% de las muestras. (noten que al ser simétrica, tenemos que alojar 2.5% de los casos “excedentes” de cada lado y para ello definimos un vector dentro de la funció qnorm (cuantiles normales)\r\n\r\n\r\nqnorm(c(0.025,0.975))\r\n\r\n\r\n[1] -1.959964  1.959964\r\n\r\nahora dibujémoslo\r\n\r\n\r\ny<-dnorm(nd)\r\n\r\nplot(nd,y,type=\"l\")\r\n  abline(v=-1.96)\r\n  abline(v=+1.96)\r\n\r\n\r\n\r\n\r\nEsto es muy importante porque si encontramos que nuestro muestreo no se ajusta a estos valores esperados entonces posiblemente pertenezcan a dos poblaciones distintas. Este es el principio básico de todas las pruebas de significancia (p=0.05), el origen de el error estándar (1.96 desv. estándar), los límites de confianza etc.\r\nTarea de hoy\r\nSupongamos que tenemos una muestra de 100 colibríes de una especie A. Les hemos medido la extensión de alas y encontramos que la media es 17 cm y la desviación estándar es de 0.8 cm.\r\nDibujen la distribución de probabilidad bajo un supuesto de normalidad\r\n¿que probabilidad hay de encontrar un caso con una extensión mayor a 18cm?\r\nNoten que cualquier valor de y en una distribución normal se puede convertir en un valor de z. Recuerden que \\(z=(y-media(y))/desv.stand\\).\r\n¿y la de encontrar un ave con una extensión menor a 15?\r\n¿ustedes creen que un ave con una extensión de 15cm puede decirse que con una confianza del 95% pertenece a la misma población?\r\n¿entre que medidas de extensión de alas se puede decir con un 95% de confianza que las aves pertenecen a esa población?\r\n¡Fin!\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:15-06:00"
    },
    {
      "path": "Estimacion2.html",
      "title": "Script Estimación 2",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nEstimación de parámetros de tendencia central\r\n\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nMedidas de dispersión\r\nC.3\r\nR.4\r\nC.4\r\nR.5\r\nC.5\r\nR.6\r\nC.6\r\nFin ———————————————————————\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nEstimación de parámetros de tendencia central\r\nSabemos que aunque los datos no se agrupen alrededor de un valor típico si hacemos muestras consecutivas los estadísticos de estas van a tener una tendencia central.\r\nVeamos un cuerpo de datos. Una variable y.\r\n\r\n\r\nyvals <- read.table(\"yvalues.txt\",header=T)\r\nattach(yvals)\r\nyvals\r\n\r\n\r\n\r\nUna manera muy simple de medir la tendencia central es ver cual es el valor más frecuente. Este se denomina MODA\r\n\r\n\r\nyord<-sort(y)\r\nyord\r\n\r\n\r\n  [1] 0.004431848 0.004431848 0.004566590 0.004566590 0.004704958\r\n  [6] 0.004704958 0.004847033 0.004847033 0.004992899 0.004992899\r\n [11] 0.005142641 0.005142641 0.005296344 0.005296344 0.005454095\r\n [16] 0.005454095 0.005615984 0.005615984 0.005782099 0.005782099\r\n [21] 0.005952532 0.005952532 0.006127377 0.006127377 0.006306726\r\n [26] 0.006306726 0.006490676 0.006490676 0.006679324 0.006679324\r\n [31] 0.006872767 0.006872767 0.007071105 0.007071105 0.007274439\r\n [36] 0.007274439 0.007482873 0.007482873 0.007696508 0.007696508\r\n [41] 0.007915452 0.007915452 0.008139809 0.008139809 0.008369689\r\n [46] 0.008369689 0.008605201 0.008605201 0.008846454 0.008846454\r\n [51] 0.009093563 0.009093563 0.009346638 0.009346638 0.009605797\r\n [56] 0.009605797 0.009871154 0.009871154 0.010142827 0.010142827\r\n [61] 0.010420935 0.010420935 0.010705598 0.010705598 0.010996937\r\n [66] 0.010996937 0.011295075 0.011295075 0.011600135 0.011600135\r\n [71] 0.011912244 0.011912244 0.012231526 0.012231526 0.012558111\r\n [76] 0.012558111 0.012892126 0.012892126 0.013233702 0.013233702\r\n [81] 0.013582969 0.013582969 0.013940061 0.013940061 0.014305109\r\n [86] 0.014305109 0.014678249 0.014678249 0.015059616 0.015059616\r\n [91] 0.015449347 0.015449347 0.015847579 0.015847579 0.016254450\r\n [96] 0.016254450 0.016670101 0.016670101 0.017094670 0.017094670\r\n[101] 0.017528300 0.017528300 0.017971133 0.017971133 0.018423311\r\n[106] 0.018423311 0.018884977 0.018884977 0.019356277 0.019356277\r\n[111] 0.019837354 0.019837354 0.020328356 0.020328356 0.020829427\r\n[116] 0.020829427 0.021340715 0.021340715 0.021862367 0.021862367\r\n[121] 0.022394530 0.022394530 0.022937354 0.022937354 0.023490985\r\n[126] 0.023490985 0.024055574 0.024055574 0.024631269 0.024631269\r\n[131] 0.025218220 0.025218220 0.025816575 0.025816575 0.026426485\r\n[136] 0.026426485 0.027048100 0.027048100 0.027681567 0.027681567\r\n[141] 0.028327038 0.028327038 0.028984661 0.028984661 0.029654585\r\n[146] 0.029654585 0.030336959 0.030336959 0.031031932 0.031031932\r\n[151] 0.031739652 0.031739652 0.032460266 0.032460266 0.033193921\r\n[156] 0.033193921 0.033940763 0.033940763 0.034700939 0.034700939\r\n[161] 0.035474593 0.035474593 0.036261869 0.036261869 0.037062910\r\n[166] 0.037062910 0.037877859 0.037877859 0.038706856 0.038706856\r\n[171] 0.039550042 0.039550042 0.040407554 0.040407554 0.041279530\r\n[176] 0.041279530 0.042166107 0.042166107 0.043067418 0.043067418\r\n[181] 0.043983596 0.043983596 0.044914772 0.044914772 0.045861076\r\n[186] 0.045861076 0.046822635 0.046822635 0.047799575 0.047799575\r\n[191] 0.048792019 0.048792019 0.049800088 0.049800088 0.050823901\r\n[196] 0.050823901 0.051863577 0.051863577 0.052919228 0.052919228\r\n[201] 0.053990967 0.053990967 0.055078902 0.055078902 0.056183142\r\n[206] 0.056183142 0.057303789 0.057303789 0.058440944 0.058440944\r\n[211] 0.059594706 0.059594706 0.060765169 0.060765169 0.061952425\r\n[216] 0.061952425 0.063156561 0.063156561 0.064377664 0.064377664\r\n[221] 0.065615815 0.065615815 0.066871091 0.066871091 0.068143566\r\n[226] 0.068143566 0.069433312 0.069433312 0.070740393 0.070740393\r\n[231] 0.072064874 0.072064874 0.073406813 0.073406813 0.074766262\r\n[236] 0.074766262 0.076143274 0.076143274 0.077537892 0.077537892\r\n[241] 0.078950158 0.078950158 0.080380109 0.080380109 0.081827776\r\n[246] 0.081827776 0.083293186 0.083293186 0.084776361 0.084776361\r\n[251] 0.086277319 0.086277319 0.087796071 0.087796071 0.089332623\r\n[256] 0.089332623 0.090886979 0.090886979 0.092459133 0.092459133\r\n[261] 0.094049077 0.094049077 0.095656796 0.095656796 0.097282269\r\n[266] 0.097282269 0.098925471 0.098925471 0.100586368 0.100586368\r\n[271] 0.102264925 0.102264925 0.103961095 0.103961095 0.105674831\r\n[276] 0.105674831 0.107406075 0.107406075 0.109154766 0.109154766\r\n[281] 0.110920835 0.110920835 0.112704207 0.112704207 0.114504800\r\n[286] 0.114504800 0.116322528 0.116322528 0.118157295 0.118157295\r\n[291] 0.120009001 0.120009001 0.121877537 0.121877537 0.123762790\r\n[296] 0.123762790 0.125664637 0.125664637 0.127582951 0.127582951\r\n[301] 0.129517596 0.129517596 0.131468430 0.131468430 0.133435304\r\n[306] 0.133435304 0.135418062 0.135418062 0.137416539 0.137416539\r\n[311] 0.139430566 0.139430566 0.141459965 0.141459965 0.143504551\r\n[316] 0.143504551 0.145564130 0.145564130 0.147638504 0.147638504\r\n[321] 0.149727466 0.149727466 0.151830800 0.151830800 0.153948287\r\n[326] 0.153948287 0.156079696 0.156079696 0.158224790 0.158224790\r\n[331] 0.160383327 0.160383327 0.162555055 0.162555055 0.164739715\r\n[336] 0.164739715 0.166937042 0.166937042 0.169146761 0.169146761\r\n[341] 0.171368592 0.171368592 0.173602247 0.173602247 0.175847430\r\n[346] 0.175847430 0.178103839 0.178103839 0.180371163 0.180371163\r\n[351] 0.182649085 0.182649085 0.184937281 0.184937281 0.187235418\r\n[356] 0.187235418 0.189543158 0.189543158 0.191860155 0.191860155\r\n[361] 0.194186055 0.194186055 0.196520499 0.196520499 0.198863119\r\n[366] 0.198863119 0.201213543 0.201213543 0.203571388 0.203571388\r\n[371] 0.205936269 0.205936269 0.208307790 0.208307790 0.210685552\r\n[376] 0.210685552 0.213069147 0.213069147 0.215458162 0.215458162\r\n[381] 0.217852177 0.217852177 0.220250767 0.220250767 0.222653499\r\n[386] 0.222653499 0.225059935 0.225059935 0.227469632 0.227469632\r\n[391] 0.229882141 0.229882141 0.232297005 0.232297005 0.234713764\r\n[396] 0.234713764 0.237131952 0.237131952 0.239551098 0.239551098\r\n[401] 0.241970725 0.241970725 0.244390351 0.244390351 0.246809491\r\n[406] 0.246809491 0.249227652 0.249227652 0.251644341 0.251644341\r\n[411] 0.254059056 0.254059056 0.256471294 0.256471294 0.258880547\r\n[416] 0.258880547 0.261286301 0.261286301 0.263688042 0.263688042\r\n[421] 0.266085250 0.266085250 0.268477402 0.268477402 0.270863972\r\n[426] 0.270863972 0.273244431 0.273244431 0.275618247 0.275618247\r\n[431] 0.277984886 0.277984886 0.280343811 0.280343811 0.282694482\r\n[436] 0.282694482 0.285036358 0.285036358 0.287368897 0.287368897\r\n[441] 0.289691553 0.289691553 0.292003780 0.292003780 0.294305030\r\n[446] 0.294305030 0.296594755 0.296594755 0.298872406 0.298872406\r\n[451] 0.301137432 0.301137432 0.303389284 0.303389284 0.305627410\r\n[456] 0.305627410 0.307851260 0.307851260 0.310060285 0.310060285\r\n[461] 0.312253933 0.312253933 0.314431657 0.314431657 0.316592908\r\n[466] 0.316592908 0.318737138 0.318737138 0.320863804 0.320863804\r\n[471] 0.322972360 0.322972360 0.325062264 0.325062264 0.327132977\r\n[476] 0.327132977 0.329183961 0.329183961 0.331214680 0.331214680\r\n[481] 0.333224603 0.333224603 0.335213199 0.335213199 0.337179944\r\n[486] 0.337179944 0.339124313 0.339124313 0.341045789 0.341045789\r\n[491] 0.342943855 0.342943855 0.344818001 0.344818001 0.346667721\r\n[496] 0.346667721 0.348492513 0.348492513 0.350291879 0.350291879\r\n[501] 0.352065327 0.352065327 0.353812370 0.353812370 0.355532529\r\n[506] 0.355532529 0.357225325 0.357225325 0.358890291 0.358890291\r\n[511] 0.360526962 0.360526962 0.362134882 0.362134882 0.363713600\r\n[516] 0.363713600 0.365262673 0.365262673 0.366781662 0.366781662\r\n[521] 0.368270140 0.368270140 0.369727684 0.369727684 0.371153879\r\n[526] 0.371153879 0.372548319 0.372548319 0.373910605 0.373910605\r\n[531] 0.375240347 0.375240347 0.376537162 0.376537162 0.377800677\r\n[536] 0.377800677 0.379030526 0.379030526 0.380226355 0.380226355\r\n[541] 0.381387815 0.381387815 0.382514571 0.382514571 0.383606292\r\n[546] 0.383606292 0.384662661 0.384662661 0.385683369 0.385683369\r\n[551] 0.386668117 0.386668117 0.387616615 0.387616615 0.388528585\r\n[556] 0.388528585 0.389403759 0.389403759 0.390241878 0.390241878\r\n[561] 0.391042694 0.391042694 0.391805971 0.391805971 0.392531483\r\n[566] 0.392531483 0.393219015 0.393219015 0.393868362 0.393868362\r\n[571] 0.394479331 0.394479331 0.395051741 0.395051741 0.395585421\r\n[576] 0.395585421 0.396080212 0.396080212 0.396535966 0.396535966\r\n[581] 0.396952547 0.396952547 0.397329832 0.397329832 0.397667706\r\n[586] 0.397667706 0.397966068 0.397966068 0.398224830 0.398224830\r\n[591] 0.398443914 0.398443914 0.398623254 0.398623254 0.398762797\r\n[596] 0.398762797 0.398862500 0.398862500 0.398922334 0.398922334\r\n[601] 0.398942280\r\n\r\n\r\n\r\n#windows()\r\nhist(y)\r\n\r\n\r\n\r\n\r\n¿cual es la clase modal aquí?\r\nPero ahora queremos saber la media aritmética (el promedio) que es la suma de todos los valores dividido por n. ¿Que hago?\r\n\r\n\r\ntotal<-sum(y)\r\nsum(y)\r\n\r\n\r\n[1] 99.73443\r\n\r\npero ahora necesito saber cuantos valores son\r\n\r\n\r\nn<-length(y)\r\nn\r\n\r\n\r\n[1] 601\r\n\r\nmedia<- total/n\r\nmedia\r\n\r\n\r\n[1] 0.1659475\r\n\r\npero ahora quiero tener una función que me sirva para siempre\r\n\r\n\r\nmedia.aritmetica <- function(x) {\r\n  sum(x)/length(x) }\r\n\r\n\r\n\r\nya está, ahora probémosla\r\n\r\n\r\ndata<-c(3,4,6,7)\r\nmedia.aritmetica(data)\r\n\r\n\r\n[1] 5\r\n\r\n\r\n\r\nmedia.aritmetica(y)\r\n\r\n\r\n[1] 0.1659475\r\n\r\n¡Que bien! ¡R es fantástico! puedo calcular la media siempre que me plazca.\r\nEn realidad la mayor parte de las funciones estadísticas están ya construidas en R. Por supuesto que la media es una de ellas. Solo quería mostrarles que no hay nada obscuro detrás de los objetos ya creados en R\r\n\r\n\r\nmean(y)\r\n\r\n\r\n[1] 0.1659475\r\n\r\nLa media como medida de tendencia central tiene el serio problema de que es muy sensible a valores atípicos. vean lo siguiente.\r\n\r\n\r\ndataat<-c(data,100)\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\nmean(dataat)\r\n\r\n\r\n[1] 24\r\n\r\ncomparado con\r\n\r\n\r\nmean(data)\r\n\r\n\r\n[1] 5\r\n\r\nUna alternativa es la mediana, que es el valor de en medio, una vez que todos los valores han sido ordenados. Veamos dataat\r\n\r\n\r\ndataat\r\n\r\n\r\n[1]   3   4   6   7 100\r\n\r\n¿Cual es la mediana?\r\n\r\n\r\nmedian(dataat)\r\n\r\n\r\n[1] 6\r\n\r\nes mucho mejor estimación de el centro que 24.\r\n¿y de data?\r\n\r\n\r\ndata\r\n\r\n\r\n[1] 3 4 6 7\r\n\r\n\r\n\r\nmedian(data)\r\n\r\n\r\n[1] 5\r\n\r\n¿ y para y?\r\n\r\n\r\nmedian(y)\r\n\r\n\r\n[1] 0.1295176\r\n\r\nmean(y)\r\n\r\n\r\n[1] 0.1659475\r\n\r\nSe parecen mucho porque no hay valores atípicos y porque la distribución es simétrica.\r\nAhora pensemos en fenómenos que cambian multiplicativamente. ¿Conocen alguno?\r\nUno de los más comunes en ecología es el crecimiento poblacional y por lo tanto la dispersión de organismos de una población. En dichos casos la media aritmética y/o la mediana suelen ser pésimos estimadores de la tendencia central. Veamos un ejemplo.\r\nEl número de insectos en una serie de plantas vecinas es\r\n\r\n\r\ninsectos<-c(1,10,1000,10,1)\r\n\r\n\r\n\r\n¿Cual es la mejor estimación de tendencia central?\r\n\r\n\r\n#windows()\r\nhist(insectos)\r\n\r\n\r\n\r\n\r\n\r\n\r\nmean(insectos)\r\n\r\n\r\n[1] 204.4\r\n\r\nmedian(insectos)\r\n\r\n\r\n[1] 10\r\n\r\nC.1\r\nR.2\r\nLo que se usa es la media geométrica que si se acuerdan hay dos maneras de calcularla. Para el ejemplo de los insectos ¿cual es la mas simple?\r\n\r\n\r\n100000^0.2\r\n\r\n\r\n[1] 10\r\n\r\n¿y la otra?\r\n\r\n\r\nexp(mean(log(insectos)))\r\n\r\n\r\n[1] 10\r\n\r\n\r\n\r\ndetach(yvals)\r\nrm(insectos)\r\nls()\r\n\r\n\r\n [1] \"A\"                \"anova\"            \"B\"               \r\n [4] \"data\"             \"dataat\"           \"EEr\"             \r\n [7] \"i\"                \"means\"            \"media\"           \r\n[10] \"media.aritmetica\" \"modelo\"           \"n\"               \r\n[13] \"nd\"               \"numeros\"          \"pearson\"         \r\n[16] \"promedios\"        \"SCA\"              \"SCE\"             \r\n[19] \"SCEa\"             \"SCEb\"             \"SCT\"             \r\n[22] \"te\"               \"total\"            \"xv\"              \r\n[25] \"y\"                \"yord\"             \"yv\"              \r\n[28] \"yvals\"           \r\n\r\nC.2\r\nR.3\r\nMedidas de dispersión\r\nVeamos un cuerpo de datos cualquiera y preguntémonos cómo podemos medir su dispersión.\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\n\r\n\r\n\r\nveamos cómo se ve\r\n\r\n\r\n#windows()\r\nplot(y,ylim=c(0,20))\r\n\r\n\r\n\r\n\r\nLo más fácil es decir de donde a donde va (el intervalo).\r\n\r\n\r\nrange(y)\r\n\r\n\r\n[1]  5 15\r\n\r\nPero esto tiene sus problemas.\r\nNo tiene relación con el parámetro población de intervalo.\r\nIncrementa con la n.\r\nAdemás es muy susceptible a valores atípicos\r\nNo considera a todos lo valores.\r\nOtra medida de dispersión muy importante es la varianza. Que está fundamentada en las desviaciones (o residuales) de cada valor con la media\r\n\r\n\r\ny<-c(13,7,5,12,9,15,6,11,9,7,12)\r\nplot(y,ylim=c(0,20))\r\nabline(mean(y),0)\r\nfor (i in 1:11) lines(c(i,i),c(y[i],mean(y)))\r\n\r\n\r\n\r\n\r\nse usa la suma de cuadrados de la diferencia de cada valor con la media general como base. ¿Cómo lo calculamos?\r\n\r\n\r\ny-mean(y)\r\n\r\n\r\n [1]  3.3636364 -2.6363636 -4.6363636  2.3636364 -0.6363636  5.3636364\r\n [7] -3.6363636  1.3636364 -0.6363636 -2.6363636  2.3636364\r\n\r\n(y-mean(y))^2\r\n\r\n\r\n [1] 11.3140496  6.9504132 21.4958678  5.5867769  0.4049587 28.7685950\r\n [7] 13.2231405  1.8595041  0.4049587  6.9504132  5.5867769\r\n\r\nsum((y-mean(y))^2)\r\n\r\n\r\n[1] 102.5455\r\n\r\nFantásitico, pero que sucede a SC cada vez que yo adiciono una nueva observación. ¿Que tenemos que hacer?\r\nSi divido entre n, se llama la desviación media de los cuadrados.\r\nC.3\r\nR.4\r\n\r\n\r\nvariance <- function (x)   sum((x-mean(x))^2)/(length(x)-1)\r\nvariance(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nPero claro, ya está definido.\r\n\r\n\r\nvar(y)\r\n\r\n\r\n[1] 10.25455\r\n\r\nLa relación entre la varianza de la muestra y el tamaño de muestra (n)\r\nLo que vamos a hacer es seleccionar aleatoriamente números de una población que tiene una distribución normal (media 10 y var 4). Esto lo vamos a hacer repetidas veces pero nuestra muestra va a ir incrementando su n desde 3 hasta 31. Vamos a sacar 30 muestras de cada tamaño de muestra. Es decir 30 muestras de 3 números, 30 muestras de 5 números etc. A cada muestra le vamos a calcular su varianza y las vamos a graficar.\r\n\r\n\r\n# windows()\r\nplot(c(0,32),c(0,15),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"Varianza\")\r\nfor (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,var(x)) }}\r\n\r\n\r\n\r\n\r\nAhora pueden ver que la varianza poblacional puede estar muy mal estimada con tamaños de muestra pequeños. Y a medida que aumentamos el tamaño de muestra la probabilidad de que la estimación está muy lejos del parámetro disminuye. Esta es una razón más para elegir muy cuidadosamente el tamaño de muestra. En unas clases vamos a hablar de esto en el contexto de pruebas de hipótesis.\r\nC.4\r\nR.5\r\nHagamos el mismo proceso que hicimos antes para elegir muestras con tamaños de muestra que incrementan pero ahora veamos que sucede con nuestra medida de desconfianza de la estimación a medida que aumenta n\r\n\r\n\r\n#windows()\r\nplot(c(0,32),c(0,2),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"Error estandar\")\r\n for (tm in seq(3,31,2)) {\r\nfor( i in 1:30){\r\nx<-rnorm(tm,mean=10,sd=2)\r\npoints(tm,sqrt(var(x)/tm)) }}\r\n\r\n\r\n\r\n\r\nVeamos un ejemplo de la concentración de ozono en unos invernaderos\r\n\r\n\r\nozono<- read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nozono\r\n\r\n\r\n\r\n\r\n\r\n#windows()\r\npar(mfrow=c(1,3))\r\nplot (gardenA)\r\nplot (gardenB)\r\nplot (gardenC)\r\n\r\n\r\n\r\n\r\n\r\n\r\nMediaA<- mean(gardenA)\r\nEEA<- sqrt(var(gardenA)/10)\r\nMediaB<- mean(gardenB)\r\nEEB<- sqrt(var(gardenB)/10)\r\nMediaC<- mean(gardenC)\r\nEEC<- sqrt(var(gardenC)/10)\r\n\r\nMediaA\r\n\r\n\r\n[1] 3\r\n\r\nEEA\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaB\r\n\r\n\r\n[1] 5\r\n\r\nEEB\r\n\r\n\r\n[1] 0.3651484\r\n\r\nMediaC\r\n\r\n\r\n[1] 5\r\n\r\nEEC\r\n\r\n\r\n[1] 1.19257\r\n\r\n¿Para cual invernadero puedo yo confiar más de la estimación de la media?\r\nC.5\r\nR.6\r\nVamos a calcular los intervalos de confianza para la media de los invernaderos usando la distribución de t. qt nos d? el valor de t para el cual hay cierta proporción a la izquierda.\r\n\r\n\r\nqt(.975,9)\r\n\r\n\r\n[1] 2.262157\r\n\r\ncalculemos los intervalos de confianza al 95% para el invernadero B\r\n\r\n\r\nqt(.975,9)*sqrt(1.3333/10)\r\n\r\n\r\n[1] 0.8260127\r\n\r\nel reporte diario, la concentración promedio de ozono en el invernadero B fue de 5.0?0.826(I.C.95%, n=10)\r\n\r\n\r\n#windows()\r\nplot(gardenB)\r\nabline(mean(gardenB),0)\r\nabline((mean(gardenB)+0.826),0, lty=2)\r\nabline((mean(gardenB)-0.826),0, lty=2)\r\n\r\n\r\n\r\n\r\nlos dejo que ustedes calculen aquellos de los invernaderos A y C.\r\nC.6\r\nFin ———————————————————————\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:18-06:00"
    },
    {
      "path": "Estimacion2bis.html",
      "title": "Script estimacion 2_bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntervalos de confianza\r\nC.6\r\n\r\nIntervalos de confianza\r\nExiste otra manera enteramente distinta de estimar intervalos de confianza, y esta es enteramente autoreferida. Se usa el método de remuestreo o bootstrapping. Veamos un cuerpo de datos.\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\nvamos a verlos visual y gráficamente\r\n\r\n\r\nvalues\r\n\r\n\r\n [1] 81.372918 25.700971  4.942646 43.020853 81.690589 51.195236\r\n [7] 55.659909 15.153155 38.745780 12.610385 22.415094 18.355721\r\n[13] 38.081501 48.171135 18.462725 44.642251 25.391082 20.410874\r\n[19] 15.778187 19.351485 20.189991 27.795406 25.268600 20.177459\r\n[25] 15.196887 26.206537 19.190966 35.481161 28.094252 30.305922\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nAhora vamos a calcular los intervalos de confianza de remuestreo por quantiles a través de la función sample que remuestrea y la función quantile basados en la muestra. Ahora, la función construida creo que no existe, así que aquí la definimos.\r\n\r\n\r\nRemuestreo<-function(x){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(x,30,replace=T))}\r\nquantile(a,c(.025,.975))}\r\nRemuestreo(values)\r\n\r\n\r\n    2.5%    97.5% \r\n24.86125 37.80309 \r\n\r\nVeamos como comparan con los IC normales\r\n\r\n\r\nmean(values)+1.96*sqrt(var(values)/30)\r\n\r\n\r\n[1] 37.53846\r\n\r\nmean(values)-1.96*sqrt(var(values)/30) \r\n\r\n\r\n[1] 24.39885\r\n\r\nVean ustedes porque elegimos 30 como una n buena para tamaño de las muestras repetidas\r\n\r\n\r\nplot(c(0,60),c(0,60),type=\"n\",xlab=\"Tamaño de muestra\",ylab=\"IC remuestreo\")\r\n \r\nfor (k in seq(5,60,3)){\r\na<-numeric(10000)\r\nfor (i in 1:10000){\r\na[i]<-mean(sample(values,k,replace=T))\r\n}\r\npoints(c(k,k),quantile(a,c(.025,.975)),type=\"b\")\r\n}\r\n\r\n# Ahora veamos como comparan con los valores de IC normales\r\nxv<-seq(5,60,0.1)\r\nyv<-mean(values)+1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\nyv<-mean(values)-1.96*sqrt(var(values)/xv)\r\nlines(xv,yv)\r\n\r\n#y los valores de IC de t student \r\n\r\nyv<-mean(values)-qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\nyv<-mean(values)+qt(.975,xv)*sqrt(var(values)/xv)\r\nlines(xv,yv,lty=2)\r\n\r\n\r\n\r\n\r\nNoten que no son exactamente simétricos. Esto hace que sean ligeramente sesgados. Existen otros dos métodos (por centiles y acelerados) que corrigen esto. También noten que para la distribución de t son más conservadores si el tamaño de muestra es pequeño.\r\nC.6\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:21-06:00"
    },
    {
      "path": "Exploratorios.html",
      "title": "Script Análisis exploratorios",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nExploración Univariada\r\nRelaciones bivariadas\r\nC.1\r\nR.2 valores faltantes\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nESTE EJEMPLO ES DE DATOS DE EVERITT 04 P. 17. CONSISTE EN INFORMACIÓN SOBRE CONTAMINACIÓN AMBIENTAL EN EU EN ZONAS METROPOLITANAS.\r\nLlamo los datos (ojo que Everitt los tiene en formato dat, si están como txt, hay que llamarlos con read.table)\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\nap <- data.frame(airpoll)\r\n# write.csv(ap, \"apdf.csv\")\r\nadf <- read.csv(\"airpoll.csv\", header = T, sep = \";\") #tabla con dato faltante\r\n\r\n\r\n\r\n\r\n\r\nattach(airpoll)\r\nairpoll\r\n\r\n\r\n         Rainfall Education Popden Nonwhite NOX SO2 Mortality\r\nakronOH        36      11.4   3243      8.8  15  59     921.9\r\nalbanyNY       35      11.0   4281      3.5  10  39     997.9\r\nallenPA        44       9.8   4260      0.8   6  33     962.4\r\natlantGA       47      11.1   3125     27.1   8  24     982.3\r\nbaltimMD       43       9.6   6441     24.4  38 206    1071.0\r\nbirmhmAL       53      10.2   3325     38.5  32  72    1030.0\r\nbostonMA       43      12.1   4679      3.5  32  62     934.7\r\nbridgeCT       45      10.6   2140      5.3   4   4     899.5\r\nbufaloNY       36      10.5   6582      8.1  12  37    1002.0\r\ncantonOH       36      10.7   4213      6.7   7  20     912.3\r\nchatagTN       52       9.6   2302     22.2   8  27    1018.0\r\nchicagIL       33      10.9   6122     16.3  63 278    1025.0\r\ncinnciOH       40      10.2   4101     13.0  26 146     970.5\r\nclevelOH       35      11.1   3042     14.7  21  64     986.0\r\ncolombOH       37      11.9   4259     13.1   9  15     958.8\r\ndallasTX       35      11.8   1441     14.8   1   1     860.1\r\ndaytonOH       36      11.4   4029     12.4   4  16     936.2\r\ndenverCO       15      12.2   4824      4.7   8  28     871.8\r\ndetrotMI       31      10.8   4834     15.8  35 124     959.2\r\nflintMI        30      10.8   3694     13.1   4  11     941.2\r\nftwortTX       31      11.4   1844     11.5   1   1     891.7\r\ngrndraMI       31      10.9   3226      5.1   3  10     871.3\r\ngrnborNC       42      10.4   2269     22.7   3   5     971.1\r\nhartfdCT       43      11.5   2909      7.2   3  10     887.5\r\nhoustnTX       46      11.4   2647     21.0   5   1     952.5\r\nindianIN       39      11.4   4412     15.6   7  33     968.7\r\nkansasMO       35      12.0   3262     12.6   4   4     919.7\r\nlancasPA       43       9.5   3214      2.9   7  32     844.1\r\nlosangCA       11      12.1   4700      7.8 319 130     861.8\r\nlouisvKY       30       9.9   4474     13.1  37 193     989.3\r\nmemphsTN       50      10.4   3497     36.7  18  34    1006.0\r\nmiamiFL        60      11.5   4657     13.5   1   1     861.4\r\nmilwauWI       30      11.1   2934      5.8  23 125     929.2\r\nminnplMN       25      12.1   2095      2.0  11  26     857.6\r\nnashvlTN       45      10.1   2082     21.0  14  78     961.0\r\nnewhvnCT       46      11.3   3327      8.8   3   8     923.2\r\nneworlLA       54       9.7   3172     31.4  17   1    1113.0\r\nnewyrkNY       42      10.7   7462     11.3  26 108     994.6\r\nphiladPA       42      10.5   6092     17.5  32 161    1015.0\r\npittsbPA       36      10.6   3437      8.1  59 263     991.3\r\nportldOR       37      12.0   3387      3.6  21  44     894.0\r\nprovdcRI       42      10.1   3508      2.2   4  18     938.5\r\nreadngPA       41       9.6   4843      2.7  11  89     946.2\r\nrichmdVA       44      11.0   3768     28.6   9  48    1026.0\r\nrochtrNY       32      11.1   4355      5.0   4  18     874.3\r\nstlousMO       34       9.7   5160     17.2  15  68     953.6\r\nsandigCA       10      12.1   3033      5.9  66  20     839.7\r\nsanfrnCA       18      12.2   4253     13.7 171  86     911.7\r\nsanjosCA       13      12.2   2702      3.0  32   3     790.7\r\nseatleWA       35      12.2   3626      5.7   7  20     899.3\r\nspringMA       45      11.1   1883      3.4   4  20     904.2\r\nsyracuNY       38      11.4   4923      3.8   5  25     950.7\r\ntoledoOH       31      10.7   3249      9.5   7  25     972.5\r\nuticaNY        40      10.3   1671      2.5   2  11     912.2\r\nwashDC         41      12.3   5308     25.9  28 102     968.8\r\nwichtaKS       28      12.1   3665      7.5   2   1     823.8\r\nwilmtnDE       45      11.3   3152     12.1  11  42    1004.0\r\nworctrMA       45      11.1   3678      1.0   3   8     895.7\r\nyorkPA         42       9.0   9699      4.8   8  49     911.8\r\nyoungsOH       38      10.7   3451     11.7  13  39     954.4\r\n\r\nnames(airpoll)\r\n\r\n\r\n[1] \"Rainfall\"  \"Education\" \"Popden\"    \"Nonwhite\"  \"NOX\"      \r\n[6] \"SO2\"       \"Mortality\"\r\n\r\nExploración Univariada\r\nComenzamos por ver el vector de medias y varianzas\r\n#mean(airpoll) #sd(airpoll)^2\r\n\r\n\r\nsummary(airpoll)\r\n\r\n\r\n    Rainfall       Education         Popden        Nonwhite    \r\n Min.   :10.00   Min.   : 9.00   Min.   :1441   Min.   : 0.80  \r\n 1st Qu.:32.75   1st Qu.:10.40   1st Qu.:3104   1st Qu.: 4.95  \r\n Median :38.00   Median :11.05   Median :3567   Median :10.40  \r\n Mean   :37.37   Mean   :10.97   Mean   :3866   Mean   :11.87  \r\n 3rd Qu.:43.25   3rd Qu.:11.50   3rd Qu.:4520   3rd Qu.:15.65  \r\n Max.   :60.00   Max.   :12.30   Max.   :9699   Max.   :38.50  \r\n      NOX              SO2           Mortality     \r\n Min.   :  1.00   Min.   :  1.00   Min.   : 790.7  \r\n 1st Qu.:  4.00   1st Qu.: 11.00   1st Qu.: 898.4  \r\n Median :  9.00   Median : 30.00   Median : 943.7  \r\n Mean   : 22.65   Mean   : 53.77   Mean   : 940.4  \r\n 3rd Qu.: 23.75   3rd Qu.: 69.00   3rd Qu.: 983.2  \r\n Max.   :319.00   Max.   :278.00   Max.   :1113.0  \r\n\r\n\r\n\r\nsummary(airpoll$SO2)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1.00   11.00   30.00   53.77   69.00  278.00 \r\n\r\nvease la diferencia entre la media y la mediana para reconocer desviaciones, calculese el intervalo intercuartiles (3er-1er).\r\n\r\n\r\n#windows()\r\nboxplot(SO2, range=0, ylab=\"SO2\") # en este caso, los \"bigotes\" del boxplot ubican el máximo (1) y el mínimo (278).\r\n\r\n\r\n\r\n\r\n\r\n\r\nboxplot(SO2, ylab=\"SO2\") #en este caso, la función se ejecuta con range = 1.5 por defecto.\r\n\r\n\r\n\r\n\r\n\r\n\r\niqSO2<-69-11\r\niqSO2\r\n\r\n\r\n[1] 58\r\n\r\nUna buena regla de dedo para identificar datos atípicos es: que los puntos que caen mas allá del 3er+1.5(intercuartil) o mas bajo que 1er-1.5(intercuartil) son valores atípicos.\r\n\r\n\r\natipicossup<-69+(iqSO2*1.5)\r\n  atipicossup\r\n\r\n\r\n[1] 156\r\n\r\n\r\n\r\natipicosinf<-abs(11-(iqSO2*1.5))\r\n  atipicosinf\r\n\r\n\r\n[1] 76\r\n\r\n\r\n\r\nhist(SO2,lwd=2)\r\nabline(v = 156, col = \"blue\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nairpoll\r\n\r\n\r\n         Rainfall Education Popden Nonwhite NOX SO2 Mortality\r\nakronOH        36      11.4   3243      8.8  15  59     921.9\r\nalbanyNY       35      11.0   4281      3.5  10  39     997.9\r\nallenPA        44       9.8   4260      0.8   6  33     962.4\r\natlantGA       47      11.1   3125     27.1   8  24     982.3\r\nbaltimMD       43       9.6   6441     24.4  38 206    1071.0\r\nbirmhmAL       53      10.2   3325     38.5  32  72    1030.0\r\nbostonMA       43      12.1   4679      3.5  32  62     934.7\r\nbridgeCT       45      10.6   2140      5.3   4   4     899.5\r\nbufaloNY       36      10.5   6582      8.1  12  37    1002.0\r\ncantonOH       36      10.7   4213      6.7   7  20     912.3\r\nchatagTN       52       9.6   2302     22.2   8  27    1018.0\r\nchicagIL       33      10.9   6122     16.3  63 278    1025.0\r\ncinnciOH       40      10.2   4101     13.0  26 146     970.5\r\nclevelOH       35      11.1   3042     14.7  21  64     986.0\r\ncolombOH       37      11.9   4259     13.1   9  15     958.8\r\ndallasTX       35      11.8   1441     14.8   1   1     860.1\r\ndaytonOH       36      11.4   4029     12.4   4  16     936.2\r\ndenverCO       15      12.2   4824      4.7   8  28     871.8\r\ndetrotMI       31      10.8   4834     15.8  35 124     959.2\r\nflintMI        30      10.8   3694     13.1   4  11     941.2\r\nftwortTX       31      11.4   1844     11.5   1   1     891.7\r\ngrndraMI       31      10.9   3226      5.1   3  10     871.3\r\ngrnborNC       42      10.4   2269     22.7   3   5     971.1\r\nhartfdCT       43      11.5   2909      7.2   3  10     887.5\r\nhoustnTX       46      11.4   2647     21.0   5   1     952.5\r\nindianIN       39      11.4   4412     15.6   7  33     968.7\r\nkansasMO       35      12.0   3262     12.6   4   4     919.7\r\nlancasPA       43       9.5   3214      2.9   7  32     844.1\r\nlosangCA       11      12.1   4700      7.8 319 130     861.8\r\nlouisvKY       30       9.9   4474     13.1  37 193     989.3\r\nmemphsTN       50      10.4   3497     36.7  18  34    1006.0\r\nmiamiFL        60      11.5   4657     13.5   1   1     861.4\r\nmilwauWI       30      11.1   2934      5.8  23 125     929.2\r\nminnplMN       25      12.1   2095      2.0  11  26     857.6\r\nnashvlTN       45      10.1   2082     21.0  14  78     961.0\r\nnewhvnCT       46      11.3   3327      8.8   3   8     923.2\r\nneworlLA       54       9.7   3172     31.4  17   1    1113.0\r\nnewyrkNY       42      10.7   7462     11.3  26 108     994.6\r\nphiladPA       42      10.5   6092     17.5  32 161    1015.0\r\npittsbPA       36      10.6   3437      8.1  59 263     991.3\r\nportldOR       37      12.0   3387      3.6  21  44     894.0\r\nprovdcRI       42      10.1   3508      2.2   4  18     938.5\r\nreadngPA       41       9.6   4843      2.7  11  89     946.2\r\nrichmdVA       44      11.0   3768     28.6   9  48    1026.0\r\nrochtrNY       32      11.1   4355      5.0   4  18     874.3\r\nstlousMO       34       9.7   5160     17.2  15  68     953.6\r\nsandigCA       10      12.1   3033      5.9  66  20     839.7\r\nsanfrnCA       18      12.2   4253     13.7 171  86     911.7\r\nsanjosCA       13      12.2   2702      3.0  32   3     790.7\r\nseatleWA       35      12.2   3626      5.7   7  20     899.3\r\nspringMA       45      11.1   1883      3.4   4  20     904.2\r\nsyracuNY       38      11.4   4923      3.8   5  25     950.7\r\ntoledoOH       31      10.7   3249      9.5   7  25     972.5\r\nuticaNY        40      10.3   1671      2.5   2  11     912.2\r\nwashDC         41      12.3   5308     25.9  28 102     968.8\r\nwichtaKS       28      12.1   3665      7.5   2   1     823.8\r\nwilmtnDE       45      11.3   3152     12.1  11  42    1004.0\r\nworctrMA       45      11.1   3678      1.0   3   8     895.7\r\nyorkPA         42       9.0   9699      4.8   8  49     911.8\r\nyoungsOH       38      10.7   3451     11.7  13  39     954.4\r\n\r\n¿Cuales son los valores atípicos para SO2? Ahora veamos lo que considera R como atípicos por default\r\n\r\n\r\npar(mfrow=c(1,3))\r\nboxplot(SO2, range=0, ylab=\"SO2\")\r\nboxplot(SO2, ylab=\"SO2\")\r\nboxplot(SO2, range=1.5, ylab=\"SO2\")\r\n\r\n\r\n\r\n\r\nVeamos las distribuciones de todas\r\n\r\n\r\npar(mfrow=c(3,3))\r\nhist(SO2,lwd=2); abline(v = c(53.77, 30), col = c(\"blue\", \"red\"))\r\nhist(Rainfall,lwd=2)\r\nhist(Education,lwd=2)\r\nhist(Popden,lwd=2)\r\nhist(Nonwhite,lwd=2)\r\nhist(NOX,lwd=2)\r\nhist(Mortality,lwd=2)\r\n\r\n\r\n\r\n\r\n¿reconocen desviaciones negativas o positivas? Son normales?\r\n\r\n\r\npar(mfrow=c(3,3))                                                 \r\nqqnorm(SO2, main=\"Q-Q plot SO2\"); qqline(SO2, col = 2, lty = 2)\r\nqqnorm(Rainfall, main=\"Q-Q plot Rainfall\"); qqline(Rainfall, col = 2, lty = 2)\r\nqqnorm(Education, main=\"Q-Q plot Education\"); qqline(Education, col = 2, lty = 2)\r\nqqnorm(Popden, main=\"Q-Q plot Popden\"); qqline(Popden, col = 2, lty = 2)\r\nqqnorm(Nonwhite, main=\"Q-Q plot Nonwhite\"); qqline(Nonwhite, col = 2, lty = 2)\r\nqqnorm(NOX, main=\"Q-Q plot NOX\"); qqline(NOX, col = 2, lty = 2)\r\nqqnorm(Mortality, main=\"Q-Q plot Mortality\"); qqline(Mortality, col = 2, lty = 2)\r\n\r\n\r\n\r\n\r\nRelaciones bivariadas\r\nVeamos que relación hay entre las distintas variables. Aquí utilizo una función smoooth (regresión con pesos locales) que permite sugerir con los propios datos que tipo de relación pudieran tener.\r\n\r\n\r\npairs(airpoll, panel=panel.smooth)\r\n\r\n\r\n\r\n\r\nveamos con mas detalle la relación SO2-mortalidad\r\n\r\n\r\nnombres<-abbreviate(row.names(airpoll))\r\npar(mfrow=c(1,1))\r\nplot(SO2,Mortality,lwd=2,type=\"n\")\r\ntext(SO2,Mortality,labels=nombres,lwd=2)\r\n\r\n\r\n\r\n\r\n\r\n\r\ndetach(airpoll)\r\n\r\n\r\n\r\nC.1\r\nR.2 valores faltantes\r\n\r\n\r\nairpoldf <- read.table(\"datofalta.txt\")\r\nairpoldf\r\nattach(airpoldf)\r\n\r\n\r\n\r\nLo mas fácil la media\r\n\r\n\r\nsummary(Mortality)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n  789.7   892.4   943.7   936.6   977.1  1112.0       1 \r\n\r\n\r\n\r\nsum(is.na(Mortality))\r\n\r\n\r\n[1] 1\r\n\r\nCual es el valor imputado? Cuales son los problemas asociados a esta imputación?\r\nregresión mortalidad y SO2\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nregmort<-lm(Mortality~SO2)\r\nsummary(regmort)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Mortality ~ SO2)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-126.625  -38.213   -7.796   35.582  196.528 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 915.4721     9.4932  96.435  < 2e-16 ***\r\nSO2           0.4266     0.1177   3.624  0.00062 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 57.47 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.1872,    Adjusted R-squared:  0.173 \r\nF-statistic: 13.13 on 1 and 57 DF,  p-value: 0.0006196\r\n\r\nm <-  (915.4720997 + (0.4266209*59))  \r\n\r\n\r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nplot(SO2,Mortality,lwd=2)\r\nabline(v = 59, h = 940.2)\r\nabline(v = 59, h = 921.9, col = \"red\")\r\nabline(lm(Mortality~SO2))\r\n\r\n\r\n\r\n\r\n\r\n\r\npredict(regmort, list(SO2=58)) \r\n\r\n\r\n       1 \r\n940.2161 \r\n\r\n\r\n\r\nlogM<-log(Mortality)\r\nlogSO2<-log(SO2+7)\r\nloglog<-lm(logM~logSO2)\r\nsummary(loglog)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = logM ~ logSO2)\r\n\r\nResiduals:\r\n      Min        1Q    Median        3Q       Max \r\n-0.136793 -0.030759  0.000398  0.029763  0.212163 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 6.749926   0.021463 314.487  < 2e-16 ***\r\nlogSO2      0.026633   0.005928   4.493 3.48e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.05863 on 57 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.2615,    Adjusted R-squared:  0.2486 \r\nF-statistic: 20.19 on 1 and 57 DF,  p-value: 3.482e-05\r\n\r\n\r\n\r\npar(mfrow=c(1,2))\r\nplot(SO2,Mortality,lwd=2) \r\nabline(regmort)\r\nabline(v = 58, h = c(940.2161, 954.4211), col = \"red\")\r\n      \r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\nabline(v = 58, h = 940.2161, col = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(logSO2,logM,lwd=2) \r\nabline(lm(logM~logSO2))\r\n\r\n\r\n\r\n\r\nel valor de SO2 que corresponde al valor faltante de mortalidad es 58. Como hemos generado un modelo de logaritmos a ambos lados de la ecuación sacamos el log del (SO2+7)\r\n\r\n\r\nlog(58+7)\r\n\r\n\r\n[1] 4.174387\r\n\r\nUsamos la función predict para predecir el valor correspondiente de Mortalidad\r\n\r\n\r\npredict(loglog, list(logSO2=4.174387)) \r\n\r\n\r\n       1 \r\n6.861105 \r\n\r\npero recordando que usamos logaritmos en el modelo, retrotransformamos con el antilog con base e (e elevado al numero que nos interesa retro transformar)\r\n\r\n\r\nexp(6.861105)\r\n\r\n\r\n[1] 954.4211\r\n\r\nEl valor predicho por regresión lineal es? Cuales son los problemas asociados a esta imputación?\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:24-06:00"
    },
    {
      "path": "index.html",
      "title": "Introducción a la estadística inferencial",
      "description": "Página del curso en su versión 2022\n",
      "author": [
        {
          "name": "Simoneta Negrete Yankelevich",
          "url": "https://www.researchgate.net/profile/Simoneta-Negrete-Yankelevich"
        },
        {
          "name": "Carlos Cultid Medina",
          "url": "https://www.researchgate.net/profile/Carlos-Cultid-Medina"
        }
      ],
      "date": "Enero 16, 2022",
      "contents": "\r\n\r\nContents\r\n¡Bienvenidos al curso!🙌\r\nCronograma del curso 📆\r\nMaterial de la clase 📚\r\nScripts de clase 📊\r\nAdicional\r\n\r\n\r\n¡Bienvenidos al curso!🙌\r\nEste es la página del curso de Introducción a la estadística inferencial, versión 2022.\r\n\r\nCronograma del curso 📆\r\n\r\nMaterial de la clase 📚\r\nEl material del curso está disponible en la página de classroom, en las siguientes ligas\r\nÉl papel de la estadística en la investigación\r\nIntroducción al Lenguaje R\r\nEstimación 1 y 2\r\nAnálisis exploratorios\r\nPrueba de hipótesis 1 y 2\r\nModelos lineales 1 y 2\r\nScripts de clase 📊\r\nLos scripts pueden ser visualizados en html en la parte superior derecha de esta página.\r\n\r\nRecuerden que las bases de datos se encuentran en el material de clase\r\nAdicional\r\nMonitor del curso 2022\r\nGabriel P. Andrade Ponce 📧 gabriel.andrade@posgrado.ecologia.edu.mx\r\nEdición de Scripts y página web: Gabriel Andrade Ponce\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:25-06:00"
    },
    {
      "path": "PanelCorrel.html",
      "title": "Panel de correlación",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPerformance Analyticis ————————————————–\r\nCorrplot —————————————————————-\r\nggcorrplot ————————————————————–\r\nGGally ——————————————————————\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nPerformance Analyticis ————————————————–\r\n\r\n\r\nairpoll<-source(\"chap2airpoll.dat\")$value\r\n\r\n\r\n\r\n\r\n\r\nlibrary(PerformanceAnalytics)\r\n\r\n\r\n\r\nchart.Correlation(log(airpoll+1),\r\n                  method=\"pearson\",\r\n                  histogram=TRUE,\r\n                  pch=20)\r\n\r\n\r\n\r\n\r\n?chart.Correlation\r\nCorrplot —————————————————————-\r\n\r\n\r\nlibrary(corrplot)\r\n\r\ncorr <- round(cor(log(airpoll+1), method = \"spearman\"),2)\r\ncor.mat <- cor.mtest(log(airpoll+1), conf.level = 0.95)\r\n\r\n\r\np1 <- corrplot(corr, method=\"color\",  \r\n         type=\"upper\", order=\"hclust\", \r\n         addCoef.col = \"black\", # Add coefficient of correlation\r\n         tl.col=\"black\", tl.srt=45, #Text label color and rotation\r\n         # Combine with significance\r\n         p.mat= cor.mat$p, sig.level = 0.01, insig = \"blank\", \r\n         # hide correlation coefficient on the principal diagonal\r\n         diag=FALSE \r\n)$corrPos      \r\ntext(p1$x, p1$y, round(p1$corr, 2))\r\n\r\n\r\n\r\n\r\nggcorrplot ————————————————————–\r\n\r\n\r\nlibrary(ggcorrplot)\r\n         \r\nggcorrplot(corr, \r\n           type = \"lower\",\r\n           lab = T, show.diag = F, \r\n           legend.title = \" Pearson\\nCorrelation\", \r\n           colors= c(\"#BB4444\", \"#FFFFFF\", \"#4477AA\"), \r\n           hc.order = T, \r\n           sig.level = 0.05, insig = \"pch\", pch=8, pch.cex = 2,  \r\n           p.mat= cor.mat$p, \r\n           ggtheme = ggplot2::theme(\r\n            panel.background = element_blank()))  \r\n\r\n\r\n\r\n\r\nGGally ——————————————————————\r\n\r\n\r\nlibrary(GGally)\r\n\r\npairs <- ggpairs(log(airpoll+1),\r\n        upper = list(continuous= wrap(\"cor\", method= \"pearson\", digits=2)),\r\n        lower = list( continuous= \"smooth\")) +theme_classic()\r\n\r\npairs\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:34-06:00"
    },
    {
      "path": "Phdospob.html",
      "title": "Script Prueba de hipótesis de dos poblaciones",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.3\r\nVolvamos al ejemplo de las concentraciones de ozono en los invernaderos y vamos a preguntarnos si el promedio de sus concentraciones de ozono es significativamente distinto\r\n\r\n\r\nozono<-read.table(\"gardens.txt\",header=T)\r\nattach(ozono)\r\nnames(ozono)\r\n\r\n\r\n\r\nVeamos un gráfico\r\n\r\n\r\nozonoAB<-c(gardenA,gardenB)\r\nozonoAB\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\netiqueta<-factor(c(rep(\"A\",10),rep(\"B\",10)))\r\netiqueta\r\n\r\n\r\n [1] A A A A A A A A A A B B B B B B B B B B\r\nLevels: A B\r\n\r\n\r\n\r\nboxplot(ozonoAB~ etiqueta, notch=T,\r\n        xlab=\"Invernadero\",ylab=\"Ozono\")\r\n\r\n\r\n\r\n\r\nSi usamos el ojimetro, parece ser que no sus medianas no son distintas porque los intervalos intercuartil no se sobrelapan. Ahora hagamos una prueba de t (para comparar las medias) a pie.\r\n¿Que necesitamos?\r\nlos grados de libertad. dijimos que para una prueba de dos poblaciones calculamos el total de observaciones (20) menos el num. de parámetros estimados antes de realizar la prueba (dos medias). Así que tenemos 18 g.l.\r\nNecesitamos calcualar las varianzas individuales de cada invernadero, para poder calcular la diferencia de EE.\r\n\r\n\r\ns2A<-var(gardenA)\r\ns2B<-var(gardenB)\r\n\r\n\r\n\r\n¿Luego que sigue?\r\nCalculamos la t de student para la diferencia de medias\r\n\r\n\r\n(mean(gardenA)-mean(gardenB))/sqrt(s2A/10+s2B/10)\r\n\r\n\r\n[1] -3.872983\r\n\r\nNoten que podemos ignorar el signo de la t. Porque solo depende que cual media pusimos primero. El valor absoluto es lo que importa.\r\nAhora necesitamos el valor crítico de la distribución de t de referencia para determinar si aceptamos o rechazamos la hipótesis de que no hay diferenicas entre medias (que vienen de la misma población). Se trata de un problema de una o dos colas?\r\nComo no me interesa cual es mayor o menor, ni tengo ninguna hipótesis de que invernadero debía tener mas o menos ozono, entonces es de dos colas, por lo tanto uso una probabilidad de?\r\n\r\n\r\nqt(0.975,18)\r\n\r\n\r\n[1] 2.100922\r\n\r\n¿Acepto o rechazo la hipótesis de que son iguales?\r\nEn virtud de que el valor calculado es mayor que el valor crítico se rechaza la hipótesis nula. (recuerden más alto=rechazo Ho más bajo=acepto)\r\nFinalmente necesito saber cual es la probabilidad de que encuentre yo la diferencia entre estas dos muestras (o una más extrema) a pesar de que provienen de poblaciones con la misma media. Recuerden que es un problema de dos colas y por esa razón necesito calcular pt y después multiplicarlos por dos (para los dos extremos).\r\n\r\n\r\np<-2*pt(-3.872983,18)\r\np\r\n\r\n\r\n[1] 0.00111454\r\n\r\npara calcular los intervalos de confianza para la diferencia medias\r\n\r\n\r\ndifmed<-mean(gardenA)-mean(gardenB)\r\nEEdifmed<-sqrt(s2A/10+s2B/10)\r\nt.de.tablas.alfa.05.gl.18<-qt(0.975,18)\r\nt.de.tablas.alfa.05.gl.18\r\n\r\n\r\n[1] 2.100922\r\n\r\n\r\n\r\nIC95<-(EEdifmed)*(t.de.tablas.alfa.05.gl.18)\r\nCotasup<- difmed+IC95\r\nCotasup\r\n\r\n\r\n[1] -0.9150885\r\n\r\n\r\n\r\nCotainf<- difmed-IC95 \r\nCotainf\r\n\r\n\r\n[1] -3.084911\r\n\r\n\r\n\r\nIC95t<-(sqrt(s2A/10+s2B/10))*(qt(0.975,18))\r\n  IC95t\r\n\r\n\r\n[1] 1.084911\r\n\r\n¿Cual es la probabilidad de que suceda? Ahora el automático\r\n\r\n\r\npruebat<-t.test(gardenA,gardenB)\r\npruebat\r\n\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  gardenA and gardenB\r\nt = -3.873, df = 18, p-value = 0.001115\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -3.0849115 -0.9150885\r\nsample estimates:\r\nmean of x mean of y \r\n        3         5 \r\n\r\nEntonces reporto la concentración de ozono fue significativamente más alta en el invernadero B (5.0 ppm) que en el A(3.0ppm; t=3.87, p=0.001(dos colas), gl=18)\r\nAhora, pudiéramos tener el problema de que las varianzas no son iguales. entonces antes de hacer una prueba de t, necesitamos preguntarnos si las varianzas son significativamente distintas. Comparemos las de los invernaderos B y C.\r\n\r\n\r\nvar(gardenB)\r\n\r\n\r\n[1] 1.333333\r\n\r\nvar(gardenC)\r\n\r\n\r\n[1] 14.22222\r\n\r\n¿Se acuerdan que la distribución de F era la adecuada para comparar varianzas? la pregunta es, ¿cual es la probabilidad de que estas dos muestras hayan sido sacada de poblaciones que tienen la misma varianza? El valor de F es simplemente el cociente de las varianzas\r\n\r\n\r\nCocienteF<-var(gardenC)/var(gardenB)\r\nCocienteF\r\n\r\n\r\n[1] 10.66667\r\n\r\nAhora comparo con la distribución de probabilidad de F para los grados de libertad correspondientes, y como no tengo ninguna idea de cual de las muestras debiera tener una varianza más alta, entonces es una prueba de dos colas.\r\nComo F no es simétrica, necesito calcular ambos lados.\r\n\r\n\r\nqf(0.975,9,9)\r\n\r\n\r\n[1] 4.025994\r\n\r\nqf(0.025,9,9)\r\n\r\n\r\n[1] 0.2483859\r\n\r\n¿Que concluyo?\r\n¿Cual es la probabilidad de que estas dos muestras provengan de poblaciones con la misma varianza?\r\n\r\n\r\n2*(1-pf(CocienteF,9,9))\r\n\r\n\r\n[1] 0.001624199\r\n\r\nAhora el automático\r\n\r\n\r\nvar.test(gardenB,gardenC)\r\n\r\n\r\n\r\n    F test to compare two variances\r\n\r\ndata:  gardenB and gardenC\r\nF = 0.09375, num df = 9, denom df = 9, p-value = 0.001624\r\nalternative hypothesis: true ratio of variances is not equal to 1\r\n95 percent confidence interval:\r\n 0.02328617 0.37743695\r\nsample estimates:\r\nratio of variances \r\n           0.09375 \r\n\r\nAhora, si encontráramos que suponer que las poblaciones de las que provienen las muestras no se distribuyen de manera normal entonces tenemos la alternativa de la prueba de Wilcoxon de suma de rangos. Esta es idéntica en su sistema a la de los rangos signados. Veamos como\r\nLo primero que hago es poner todas las observaciones en un mismo vector\r\n\r\n\r\nozone<-c(gardenA,gardenB)\r\nozone\r\n\r\n\r\n [1] 3 4 4 3 2 3 1 3 5 2 5 5 6 7 4 4 3 5 6 5\r\n\r\nAhora les hago sus etiquetas para que no se me pierdan\r\n\r\n\r\netiqueta<-c(rep(\"A\",10),rep(\"B\",10))\r\netiqueta\r\n\r\n\r\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\r\n[17] \"B\" \"B\" \"B\" \"B\"\r\n\r\nAhora hago un vector de los rangos con la función rank\r\n\r\n\r\nrangoscomb<-rank(ozone)\r\nrangoscomb\r\n\r\n\r\n [1]  6.0 10.5 10.5  6.0  2.5  6.0  1.0  6.0 15.0  2.5 15.0 15.0 18.5\r\n[14] 20.0 10.5 10.5  6.0 15.0 18.5 15.0\r\n\r\nNoten que para todos los valores repetidos se hace un promedio de los rangos que les tocan.\r\n\r\n\r\ntapply(rangoscomb,etiqueta,sum)\r\n\r\n\r\n  A   B \r\n 66 144 \r\n\r\nFinalmente uso el valor más pequeño (66) para compararlo con el valor de tablas para el etadístico W para n de 10 y 10 y un alfa del 5% (W=78). Como nuestro valor es menor (porque uso como referencia el mas pequeño del par que obtuve), entonces rechazo mi H0. Las Medias son significativamente distintas.\r\nAhora el automático.\r\n\r\n\r\nwilcox.test(gardenA,gardenB)\r\n\r\n\r\n\r\n    Wilcoxon rank sum test with continuity correction\r\n\r\ndata:  gardenA and gardenB\r\nW = 11, p-value = 0.002988\r\nalternative hypothesis: true location shift is not equal to 0\r\n\r\nLas diferencias se deben a que R utiliza un algoritmo de aproximación para calcular valores de z. Es ligeramente distinto del tradicional que hicimos arriba. La mecánica es la misma. Noten la diferencia en las p de t y W. Wilcoxon es menos poderoso (95%). Pero es el correcto si las distribuciones son sesgadas. También es correcto si no lo son (aunque poco menos poderoso). t no es correcta si hay desviaciones sustanciales de la normalidad.\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:37-06:00"
    },
    {
      "path": "Phdospobbis.html",
      "title": "Script Prueba de hipótesis de dos poblaciones-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.3\r\nC.3\r\nTarea\r\nAquí va la segunda parte de su tarea\r\n\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.3\r\nEntonces ¿cual es la probabilidad de que mi muestra de frecuencias (o una mas extrema) pertenezca a una población donde el aislamiento de la planta y el plumaje del colibrí defensor son independientes?\r\n\r\n\r\n1-pchisq(35.34,1)\r\n\r\n\r\n[1] 2.768866e-09\r\n\r\n¿Cual es el valor crítico para rechazar la H0 con un alfa de 0.05?\r\n\r\n\r\nqchisq(0.95,1)\r\n\r\n\r\n[1] 3.841459\r\n\r\nAhora vamos a hacerlo de manera automática en R\r\n\r\n\r\ncolibries<-matrix(c(38,14,11,51),nrow=2)\r\ncolibries\r\n\r\n\r\n     [,1] [,2]\r\n[1,]   38   11\r\n[2,]   14   51\r\n\r\nchisq.test(colibries)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test with Yates' continuity correction\r\n\r\ndata:  colibries\r\nX-squared = 33.112, df = 1, p-value = 8.7e-09\r\n\r\nNoten que estos valores son un poco distintos porque se aplicó una corrección de Yates. Esta fue diseñada para frecuencias pequeñas, pero ahora existen pruebas mejores para frecuencias pequeñas (20% o más frecuencias menores a 5) como La Prueba exacta de Fisher Crawley p. 90 QyK p.388.\r\nEntonces le quito la corrección\r\n\r\n\r\nchisq.test(colibries,correct=F)\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  colibries\r\nX-squared = 35.334, df = 1, p-value = 2.778e-09\r\n\r\nme da exactamente lo que calculamos a mano.\r\nC.3\r\nTarea\r\nAquí va la segunda parte de su tarea\r\nEl problema se trata de un investigador que esta interesado en si las hormigas construyen preferentemente sus nidos en árboles de una de dos especies. Entonces muestreó 100 árboles de cada una de las especies. Encontró que 60 árboles de la especie A y 20 de la especie B tenían nidos. ¿Esta muestra apoya la hipótesis de preferencia?\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:37-06:00"
    },
    {
      "path": "Phunapob.html",
      "title": "Script Prueba de hipótesis de una poblacion",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nVeamos un ejemplo de prueba de hipótesis para una población (Crawley p.64). Estos son datos de Michelson (1978), de medidas tomadas para estimar la velocidad de la luz. Que ahora sabemos es cercano (299,990 km p seg).\r\n\r\n\r\nveluz<-read.table(\"light.txt\",header=T)\r\nattach(veluz)\r\nnames(veluz)\r\n\r\n\r\n\r\n\r\n\r\nhist(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(speed)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n    650     850     940     909     980    1070 \r\n\r\nA todos los valores se les ha restado 299 000 para facilitar su visualización ¿que pueden ver aquí? ¿tenemos valores atípicos?\r\n\r\n\r\nboxplot(speed)\r\n\r\n\r\n\r\n\r\n\r\n\r\nqqnorm(speed)\r\nqqline(speed,lty=2)\r\n\r\n\r\n\r\n\r\nLa muestra no se distribuye de acuerdo a lo esperado normalmente (cosa que una prueba de t para una población asume), pero además ¿Creen que la población de referencia se distribuya de manera normal?\r\nNo puede porque tiene el problema de que la vel de la luz no puede tomar valores negativos\r\nNuestra hipótesis es que los datos de Michelson difieren de el valor prevaleciente en esa época como la vel de la luz 299,990 km/seg. Como a todos los valores les quitaron 299 000 entonces ¿cual va a ser la referencia?\r\nAhora, si cumpliéramos con el supuesto de que la pob. de referencia se distribuye normalmente, cómo resolvemos este problema?…ustedes ya lo saben hacer.\r\nPrimero. ¿Este problema tiene una o dos colas que le pisen? Entonces ¿que harían?\r\nBueno, pero como no cumplimos con el primero de los supuestos, necesitamos otra alternativa. Que se les ocurre?\r\nPor supuesto que también podemos usar una técnica de remuestreo con remplazo! Este es el punto 1. de la tarea de hoy. Hagan la prueba con bootstrap\r\nExiste otra alternativa es una prueba llamada de rangos signados de Wilcoxon.\r\nC.1\r\nR.2\r\n\r\n\r\nlibrary(stats)\r\n\r\nwilcox.test(speed,mu=990)\r\n\r\n\r\n\r\n    Wilcoxon signed rank test with continuity correction\r\n\r\ndata:  speed\r\nV = 22.5, p-value = 0.00213\r\nalternative hypothesis: true location is not equal to 990\r\n\r\nLa probabilidad de obtener la media de nuestra muestra (estadístico) en una población de las medias de muchas muestras con media 990 (parámetro) es del 0.2%. Como aceptamos una probabilidad de equivocarnos al rechazar una H0 cuando esta es cierta del 5% entonces, la rechazamos!\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:39-06:00"
    },
    {
      "path": "Phunapobbis.html",
      "title": "Script Prueba de hipótesis de una poblacion-bis",
      "author": [],
      "contents": "\r\n\r\nContents\r\nC.2\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nC.2\r\nVamos a escribir una función para calcular sesgo.\r\n\r\n\r\nskew<-function(x){\r\nm3<-sum((x-mean(x))^3)/length(x)\r\ns3<-sqrt(var(x))^3\r\nm3/s3  }\r\n\r\n\r\n\r\nveamos los datos skewdata\r\n\r\n\r\ndata<-read.table(\"skewdata.txt\",header=T)\r\nattach(data)\r\nnames(data)\r\n\r\n\r\n\r\n\r\n\r\nhist(values)\r\n\r\n\r\n\r\n\r\nCalculemos el sesgo\r\n\r\n\r\nskew(values)\r\n\r\n\r\n[1] 1.318905\r\n\r\nAhora hagamos una prueba de t para determinar si hay sesgo respecto a lo esperado para una población de inferencia con distribución normal\r\n\r\n\r\nskew(values)/sqrt(6/length(values))\r\n\r\n\r\n[1] 2.949161\r\n\r\n1-pt(2.949,28)\r\n\r\n\r\n[1] 0.003185136\r\n\r\n¿Cual es la conclusión?. Efectivamente está más sesgado de los esperado normalmente, ¿que se puede hacer, si insistimos en cumplir con la normalidad?\r\n\r\n\r\nskew(sqrt(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] 1.474851\r\n\r\nskew(log(values))/sqrt(6/length(values))\r\n\r\n\r\n[1] -0.6600605\r\n\r\n\r\n\r\nkurtosis<-function(x) {\r\nm4<-sum((x-mean(x))^4)/length(x)\r\ns4<-var(x)^2\r\nm4/s4 - 3  }\r\nkurtosis(values)\r\n\r\n\r\n[1] 1.297751\r\n\r\nkurtosis(values)/sqrt(24/length(values))\r\n\r\n\r\n[1] 1.45093\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:40-06:00"
    },
    {
      "path": "Regresionsimp.html",
      "title": "Script Modelos lineales",
      "author": [],
      "contents": "\r\n\r\nContents\r\nR.1\r\nC.1\r\nR.2\r\nC.2\r\nR.3\r\nC.3\r\nR.4\r\nFin\r\n\r\n\r\n\r\n Descargar script\r\n\r\n\r\n\r\n\r\n Descargar script en pdf\r\n\r\n\r\nR.1\r\nLlamo los datos, los coloco en un dataframe y convierto las columnas en variables\r\n\r\n\r\nreg.data<-read.table(\"tannin.txt\",header=T)\r\nattach(reg.data)\r\nnames(reg.data)\r\n\r\n\r\n\r\nGráfico\r\n\r\n\r\npar(mfrow=c(1,1))\r\nplot(tannin,growth,pch=16)\r\n\r\n\r\n\r\n\r\n¿La tendencia de la variable de respuesta es a incrementar o a disminuir con la explicatoria? tendencia a disminuir\r\n¿Es factible que los datos sean explicados por una línea horizontal? H0.\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\n\r\n\r\n\r\n\r\nLa H0 no parece factible, entonces b es probablemente dif de 0 y negativa\r\n¿Si existe una tendencia es recta o curva? relación recta, entonces proponemos el modelo \\(y=a+bx+e\\)\r\n¿La dispersión de los datos es uniforme a lo largo de la línea o cambia sistemáticamente con la variable explicatoria?. Dispersión muy uniforme, la ordenada al origen es dif de 0 entonces a es prob mayor que 0.\r\n¿A ojo cuales son los valores de a y b? Como podemos hacer este proceso sistemático y preciso?\r\nC.1\r\nR.2\r\nY la variación total de y es la dispersión de los datos alrededor de y barra.\r\nLa Suma de Cuadrados Total es \\(SCT= \\sum(y-\\overline{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16)\r\nabline(mean(growth),0)\r\nfor (i in 1:9) lines(c(tannin[i],tannin[i]),c(growth[i],mean(growth)))\r\n\r\n\r\n\r\n\r\nLa mejor recta ajustada por el método de mínimos cuadrados es aquella que minimiza la Suma de Cuadrados de las desviaciones de los valores de y de la línea ajustada \\(\\hat{y}\\), \\(SCE=\\sum(y - \\hat{y})^2\\)\r\n\r\n\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\n\r\n\r\n\r\n\r\n\r\n\r\nysomb <- predict(lm(growth ~ tannin))\r\nplot(tannin,growth,pch=16) \r\nabline(lm(growth~tannin))\r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(growth[i], ysomb[i]))\r\n\r\n\r\n\r\n\r\nC.2\r\nR.3\r\nAhora bien, una tercera cantidad es la Suma de Cuadrados de la Regresión (es decir del efecto de la variable predictora)\r\n\\[SCR = SCTotal - SCError\\]\r\n\r\n\r\nplot(tannin, growth, type = \"n\") \r\nabline(mean(growth), 0) \r\nmodelito <- lm(growth ~ tannin) \r\nabline(modelito) \r\nfor(i in 1:9) lines(c(tannin[i], tannin[i]), c(mean(growth), predict(modelito)[i])) \r\npoints(tannin,predict(modelito), pch = 16) \r\npoints(tannin, growth)\r\n\r\n\r\n\r\n\r\nC.3\r\nR.4\r\nEmpezamos a ajustar los modelos: modelo nulo - solo la media\r\n\r\n\r\nNulo <- lm(growth ~ 1) \r\nnames(Nulo)\r\n\r\n\r\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \r\n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \r\n [9] \"call\"          \"terms\"         \"model\"        \r\n\r\n\r\n\r\nanova(Nulo) \r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value Pr(>F)\r\nResiduals  8 108.89  13.611               \r\n\r\n\r\n\r\nNulo$df.residual \r\n\r\n\r\n[1] 8\r\n\r\nNulo$coefficients \r\n\r\n\r\n(Intercept) \r\n   6.888889 \r\n\r\nNulo$fitted.values\r\n\r\n\r\n       1        2        3        4        5        6        7 \r\n6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 6.888889 \r\n       8        9 \r\n6.888889 6.888889 \r\n\r\n\r\n\r\nplot(tannin, growth) \r\nabline(a=Nulo$coe, b=0) \r\nabline(Nulo$coe, 0)\r\n\r\n\r\n\r\n\r\nEs decir solo se ha ajustado la media que no ofrece información importante\r\nAgregamos el efecto del tannin\r\n\r\n\r\nTanino <- update(Nulo, . ~ . + tannin)\r\nTanino\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nCoefficients:\r\n(Intercept)       tannin  \r\n     11.756       -1.217  \r\n\r\n\r\n\r\nanova(Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: growth\r\n          Df Sum Sq Mean Sq F value    Pr(>F)    \r\ntannin     1 88.817  88.817  30.974 0.0008461 ***\r\nResiduals  7 20.072   2.867                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nTanino$coefficients\r\n\r\n\r\n(Intercept)      tannin \r\n  11.755556   -1.216667 \r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nO bien pedimos la secuencia de ajustes, que produce estos cambios en devianza\r\n\r\n\r\nanova(Nulo, Tanino)\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: growth ~ 1\r\nModel 2: growth ~ tannin\r\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \r\n1      8 108.889                                  \r\n2      7  20.072  1    88.817 30.974 0.0008461 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\n\r\nsummary(Tanino)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth ~ tannin)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4556 -0.8889 -0.2389  0.9778  2.8944 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.7556     1.0408  11.295 9.54e-06 ***\r\ntannin       -1.2167     0.2186  -5.565 0.000846 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.693 on 7 degrees of freedom\r\nMultiple R-squared:  0.8157,    Adjusted R-squared:  0.7893 \r\nF-statistic: 30.97 on 1 and 7 DF,  p-value: 0.0008461\r\n\r\nSi queremos, podemos guardar los valores ajustados y los residuales en la base de datos:\r\n\r\n\r\nreg.data$ajustados <- fitted.values(Tanino) \r\nreg.data$residuales <- residuals(Tanino)\r\nreg.data\r\n\r\n\r\n  growth tannin ajustados residuales\r\n1     12      0 11.755556  0.2444444\r\n2     10      1 10.538889 -0.5388889\r\n3      8      2  9.322222 -1.3222222\r\n4     11      3  8.105556  2.8944444\r\n5      6      4  6.888889 -0.8888889\r\n6      7      5  5.672222  1.3277778\r\n7      2      6  4.455556 -2.4555556\r\n8      3      7  3.238889 -0.2388889\r\n9      3      8  2.022222  0.9777778\r\n\r\nPara inspeccionar qué tan bueno es el modelo existen algunos recursos gráficos donde se examinan la distribución de los residuales y los puntos extremos que que pueden “cargar” el valor numérico de los parámetros:\r\n\r\n\r\npar(mfcol=c(2,2))\r\nplot(Tanino)\r\n\r\n\r\n\r\n\r\nExaminamos un modelo sin el dato extremo:\r\n\r\n\r\nSindat7 <- lm(growth[-7] ~ tannin[-7]) \r\nsummary(Sindat7)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = growth[-7] ~ tannin[-7])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.4549 -0.9572 -0.1622  0.4572  2.6622 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  11.6892     0.8963  13.042 1.25e-05 ***\r\ntannin[-7]   -1.1171     0.1956  -5.712  0.00125 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.457 on 6 degrees of freedom\r\nMultiple R-squared:  0.8446,    Adjusted R-squared:  0.8188 \r\nF-statistic: 32.62 on 1 and 6 DF,  p-value: 0.001247\r\n\r\nNo ganamos gran cosa\r\nPara predecir valores usamos:\r\n\r\n\r\npredict(Tanino, list(tannin =7.5))\r\n\r\n\r\n       1 \r\n2.630556 \r\n\r\n\r\n\r\npar(mfrow=c(1,1)) \r\nls() \r\n\r\n\r\n [1] \"a\"                         \"A\"                        \r\n [3] \"adf\"                       \"airpoldf\"                 \r\n [5] \"airpoll\"                   \"anova\"                    \r\n [7] \"ap\"                        \"atipicosinf\"              \r\n [9] \"atipicossup\"               \"B\"                        \r\n[11] \"CocienteF\"                 \"colibries\"                \r\n[13] \"cor.mat\"                   \"corr\"                     \r\n[15] \"Cotainf\"                   \"Cotasup\"                  \r\n[17] \"data\"                      \"dataat\"                   \r\n[19] \"difmed\"                    \"EEA\"                      \r\n[21] \"EEB\"                       \"EEC\"                      \r\n[23] \"EEdifmed\"                  \"EEr\"                      \r\n[25] \"etiqueta\"                  \"i\"                        \r\n[27] \"IC95\"                      \"IC95t\"                    \r\n[29] \"iqSO2\"                     \"k\"                        \r\n[31] \"kurtosis\"                  \"loglog\"                   \r\n[33] \"logM\"                      \"logSO2\"                   \r\n[35] \"m\"                         \"means\"                    \r\n[37] \"media\"                     \"media.aritmetica\"         \r\n[39] \"MediaA\"                    \"MediaB\"                   \r\n[41] \"MediaC\"                    \"modelito\"                 \r\n[43] \"modelo\"                    \"n\"                        \r\n[45] \"nd\"                        \"nombres\"                  \r\n[47] \"Nulo\"                      \"numeros\"                  \r\n[49] \"ozone\"                     \"ozono\"                    \r\n[51] \"ozonoAB\"                   \"p\"                        \r\n[53] \"p1\"                        \"pairs\"                    \r\n[55] \"pearson\"                   \"promedios\"                \r\n[57] \"pruebat\"                   \"rangoscomb\"               \r\n[59] \"reg.data\"                  \"regmort\"                  \r\n[61] \"Remuestreo\"                \"s2A\"                      \r\n[63] \"s2B\"                       \"SCA\"                      \r\n[65] \"SCE\"                       \"SCEa\"                     \r\n[67] \"SCEb\"                      \"SCT\"                      \r\n[69] \"Sindat7\"                   \"skew\"                     \r\n[71] \"t.de.tablas.alfa.05.gl.18\" \"Tanino\"                   \r\n[73] \"te\"                        \"tm\"                       \r\n[75] \"total\"                     \"variance\"                 \r\n[77] \"veluz\"                     \"x\"                        \r\n[79] \"xv\"                        \"y\"                        \r\n[81] \"yord\"                      \"ysomb\"                    \r\n[83] \"yv\"                        \"yvals\"                    \r\n\r\nrm(list=ls(all=TRUE))\r\n\r\n\r\n\r\nFin\r\n\r\n\r\n\r\n",
      "last_modified": "2022-01-16T23:50:43-06:00"
    }
  ],
  "collections": []
}
